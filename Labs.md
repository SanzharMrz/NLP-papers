# Labs variants

Here is a table of NLP Labs for applying to research fellowship
| Lab Name                           | Lab Info                                                                                                                                                                                                                          | Lab Vector                                                                                                     | Link          |
|------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|---------------|
| IAPS: Institute of AI Policy and Strategy | Conduct policy research, looking over the horizon to identify policy proposals that are actionable today but relevant tomorrow. Cultivate policy talent, forging a community of researchers and practitioners who are thoughtful about uncertainty but able to get things done. | AI Safety / AI Security / Securing benefits and managing risks from advanced AI                                | [link](#)     |
| GOV.AI: Governance AI              | Responsible Development: How can general-purpose AI developers make responsible development and deployment decisions? Regulation: How can governments use their regulatory toolboxes to ensure that AI developers and users behave responsibly? International Governance: What role can international coordination play in reducing risks from AI? Compute Governance: How can public and private decisions about access to compute shape outcomes? The central focus of their research is threats that general-purpose AI systems may pose to security. They seek to understand the risks they pose today, while also looking ahead to the more extreme risks they could pose in the future. | AI Governance / AI Safety / Risk Management                                                                    | [link](#)     |
| SPAR: Supervised Program For Alignment | SPAR is a virtual, part-time, volunteer research program designed to give early career individuals and professionals an opportunity to work on a AI safety project for 3 months. There is an average time commitment of 5 to 20 hours per week for mentees and 2-10 hours per week for mentors. | SPAR provides a unique opportunity for early-career individuals and professionals to contribute to AI safety research by participating in mentorship, either as a mentor or mentee, in technical or governance research. | [link](#)     |
| Stanford HAI: Human-Centered AI   | The Stanford Institute for Human-Centered Artificial Intelligence (HAI) is dedicated to advancing AI research, education, policy, and practice to improve the human condition. They focus on studying and developing AI technologies that are human-centered and benefit society. | Explainable AI / Human-Centered AI / AI Ethics                                                                 | [link](https://hai.stanford.edu) |
| MIT-IBM Watson AI Lab              | A collaboration between MIT and IBM focused on fundamental AI research, including interpretability, robustness, and trust in AI systems. They work on projects that ensure AI systems are transparent and their decisions understandable. | Explainable AI / Trustworthy AI / Robustness in AI                                                             | [link](https://mitibmwatsonailab.mit.edu) |
| XAI Lab @ DARPA                    | The Explainable Artificial Intelligence (XAI) program aims to create a suite of machine learning techniques that produce more explainable models while maintaining a high level of learning performance (prediction accuracy) and enable human users to understand, appropriately trust, and effectively manage the emerging generation of AI systems. | Explainable AI / Human-AI Interaction / AI Transparency                                                        | [link](https://www.darpa.mil/program/explainable-artificial-intelligence) |
| FAccT: Fairness, Accountability, and Transparency in AI | Focuses on the fairness, accountability, and transparency of AI systems. They research ways to make AI systems more transparent and accountable, ensuring they operate fairly and ethically.                                         | AI Fairness / AI Accountability / Transparency in AI                                                           | [link](https://facctconference.org) |
| USC CAIS: Center for AI in Society | A collaboration between the University of Southern Californiaâ€™s School of Social Work and Viterbi School of Engineering. They work on AI projects that aim to address societal challenges, focusing on creating transparent and explainable AI systems. | AI for Social Good / Explainable AI / AI Ethics                                                                | [link](https://cais.usc.edu) |
| Berkeley BAIR: Berkeley Artificial Intelligence Research | The Berkeley Artificial Intelligence Research (BAIR) Lab brings together researchers across computer science, statistics, and operations research to address fundamental challenges in AI. They focus on areas such as interpretability, fairness, and transparency of AI systems. | Explainable AI / Fairness in AI / AI Transparency                                                              | [link](https://bair.berkeley.edu) |
| FAIR: Facebook AI Research         | Facebook AI Research (FAIR) is dedicated to advancing the field of machine intelligence and developing technologies that give people better ways to communicate. They focus on building AI systems that are transparent and explainable. | Explainable AI / AI Transparency / AI Communication                                                            | [link](https://ai.facebook.com) |
| DeepMind: DeepMind XAI             | DeepMind focuses on developing AI systems with the goal of advancing science and benefiting humanity. Their research includes efforts to make AI systems more understandable and transparent to users. | Explainable AI / AI for Science / AI Transparency                                                              | [link](https://deepmind.com) |
| Microsoft Research AI              | Microsoft Research AI is a research division that focuses on AI's various aspects, including its interpretability, fairness, and security. They work on making AI systems that are transparent and accountable. | Explainable AI / AI Fairness / AI Security                                                                     | [link](https://www.microsoft.com/en-us/research/lab/microsoft-research-ai) |
| CMU LTI: Carnegie Mellon University Language Technologies Institute | CMU's LTI focuses on developing technologies to improve human language understanding and processing. They work on making AI systems that can explain their decisions in human-understandable terms. | Explainable AI / Natural Language Processing / AI Ethics                                                       | [link](https://www.lti.cs.cmu.edu) |



