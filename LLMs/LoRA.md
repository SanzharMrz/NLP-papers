# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS

Authours propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times.

# Conception

![image](https://github.com/SanzharMrz/NLP-papers/assets/48170101/d0c63624-5dda-4958-a334-e99db5897343)

![image](https://github.com/SanzharMrz/NLP-papers/assets/48170101/fc35bb42-1a77-4523-8df2-9655e6638879)

![image](https://github.com/SanzharMrz/NLP-papers/assets/48170101/454a2548-8d79-41ea-9dd3-9015796b6203)


# Related works

Placeholder

# Metrics

Placeholder
